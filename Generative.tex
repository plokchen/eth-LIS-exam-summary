% -*- root: Main.tex -*-
\section*{Discriminative vs. Generative Modeling}
Discriminative models: aim to estimate $P(y|x)$\\
generative models:  aim to estimate joint distribution $P(y,x)$\\

Typical approach:
\begin{itemize}
	\item Estimate prior on labels $P(y)$
	\item Estimate conditional distribution for each class y $P(x|y)$
	\item Obtain predictive distribution using Bayes rule $P(y|x) = \frac{1}{Z} P(y) P(x|y)$
\end{itemize}

\subsection*{Example MLE for P(y)}
Want: $p = P(Y=1), P(y=-1) = 1-p$\\
Given: $D=\{(y_1,x_1),...,(y_n,x_n)\}, D_y=\{y_1,...,y_n\}$\\
$P(D_y|p) = \prod_{i=1}^n p^{[y_i=+1]} (1-p)^{[y_i=-1]}$\\
$=p^{n_+} (1-p)^{n_-}$ where $n_+ = $ # of $y=+1$\\
$\frac{\partial}{\partial p} log P(D_y|p) = n_+ \frac{1}{p} - n_- \frac{1}{1-p} \overset{!}{=} 0 \Rightarrow p=\frac{n_+}{n_+ + n_-}$

\subsection*{Example MLE for P=(x|y)}
Assume: $P(X=x_i|y) = \mathcal{N}(x_i;\mu_{i,y}, \sigma_{i,y}^2)$\\
Given: $D, D_{x_i|y} = \{x \text{, s.t. } x_{j,i}=x, y_i=y\}$\\
Thus MLE yields:\\
$\mu_{i,y} = \frac{1}{n_y} \sum_{x\in D_{x_i|y}} x$, where $n_y=|D_{x_i|y}|$\\
$\sigma_{i,y}^2 = \frac{1}{n_y} \sum_{x\in D_{x_i|y}} (x-\mu_{i,y})^2$

\subsection*{Deriving decision rule}
In order to predict label y for new point x, use\\
$P(y|x) = \frac{1}{Z} P(y)P(x|y)$\\
$y = \underset{y}{\operatorname{argmax}} P(y|x)$\\
$= \underset{y}{\operatorname{argmax}} P(y) \prod_{i=1}^d P(x_i|y)$\\
$= \underset{y}{\operatorname{argmax}} log P(y) + \sum_{i=1}^d log P(x_i|y)$

\subsection*{Example: Gaussian Naive Bayes classifier}
MLE for class prior: $P(Y=y) = p_y = \frac{\operatorname{Count(Y = y)}}{n}$\\
MLE for feature distribution: $P(X_1,...,X_d|Y) = \prod_{i=1}^d P(X_i|Y) = \mathcal{N}(x_i|\mu_{y,i}, \sigma_{y,i}^2)$\\
$\mu_{y,i} = \frac{1}{\operatorname{Count}(Y=y)} \sum_{j:y_j=y} x_j,i$\\
$\sigma_{y,i}^2 = \frac{1}{\operatorname{Count}(Y=y)} \sum_{j:y_j=y} (x_{j,i} - \mu_{y,i})^2$\\
Prediction given new point\\
$y = \underset{y}{\operatorname{argmax}} P(y|x) = \underset{y}{\operatorname{argmax}} P(y) \prod_{i=1}^d P(x|y)$

\subsection*{Example: Gaussian Bayes Classifier}
MLE for class prior: $P(Y=y) = p_y = \frac{\operatorname{Count(Y = y)}}{n}$\\
MLE for feature distribution: $P(x|Y) = \mathcal{N}(x ; \mu_y \Sigma_y)$
%\textcolor{red}{TODO: Is this correct?(X's are not assumed to be independent, rigth?) I don't think so. See slide 23,24 of lecture 20} Changed.. \\
$\hat{\mu}_{y} = \frac{1}{\operatorname{Count}(Y=y)} \sum_{i:y_i=y} x_i \in \mathbb{R}^d$\\
$\hat{\Sigma}_{y} = \frac{1}{\operatorname{Count}(Y=y)} \sum_{i:y_i=y} (x_i - \hat{\mu}_{y})(x_i-\hat{\mu}_y)^T \in \mathbb{R}^{d \times d}$\\

\subsection*{Categorical Naive Bayes Classifier}
MLE class prior: $P(Y=y) = \frac{Count(Y=y)}{n}$\\
MLE for feature dist.:\\
$P(X_i = c|y) = \frac{Count(X_i = c, Y = y)}{Count(Y=y)}$


\subsection*{Outlier Detection}
$P(x) = \sum_y P(x,y) = \sum_y P(y)P(x|y) \leq \tau$