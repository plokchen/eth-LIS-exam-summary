% -*- root: Main.tex -*-
\section*{Probability Modeling}
\subsection*{Goal}
Given data $D=\{(x_1,y_1), ..., (x_n,y_n)\} \subseteq X \times Y$\\
Want to find the hypothesis with the minimum prediction error (risk). $R(h) = \int P(x,y)l(y;h(x)) dxdy = \mathbb{E}_{x,y}[l(y;h(x))]$\\
Fundamental assumption: $(x_i,y_i) \in_{iid} X \times Y$\\
Hypothesis: is a conditional mean $H^*(x) = \mathbb{E}[y|X=x]$
\subsection*{Maximum Likelihood Estimation (MLE)}
Choose a particular parametric form $P(Y|X,\theta)$, then optimize the parameters using Maximum Likelihood Estimation.

$\theta^* = \underset{\theta}{\operatorname{argmax}} P(y|x,\theta) $\\
$= \underset{\theta}{\operatorname{argmax}} \prod_{i=1}^n P(y_i|x_i, \theta) \text{\quad (iid)}$\\
$= \underset{\theta}{\operatorname{argmin}} - \sum_{i=1}^n log P(y_i|x_i,\theta)$\\

\subsection*{Example: MLE for lin. Gaussian}
Assume: $P(Y=y|X=x, \theta) = \mathcal{N}(y;h(x),\sigma^2)$\\
and $h(x) = w^T x$ is linear\\
Equivalent: $Y=w^T X + \epsilon, \epsilon \in \mathcal{N}(0, \sigma^2) \Leftrightarrow$\\
$y_i \in \mathcal{N} (w^T x_i, \sigma^2)$\\
Maximizing the log likelihood:\\
$\underset{w}{\operatorname{argmax}} P(y|x,\theta) = \underset{w}{\operatorname{argmax}} \prod \limits_i \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2} \frac{(y_i-h(x_i))^2}{\sigma^2}}$\\
log is monotonic and cancel all constants\\
$= \underset{w}{\operatorname{argmin}}  \sum_i (y_i-w^Tx_i)^2$

\subsection*{Bias/Variance/Noise}
Prediction error = $Bias^2 + Variance + Noise$

\subsection*{Maximum a posteriori estimate (MAP)}
Introduce bias by expressing assumption through a Bayesian prior $w_i \in \mathcal{N}(0, \beta^2)$\\
Bayes rule: $P(w|x,y) = \frac{P(w|x) P(y|x,w)}{P(y|x)}$\\
$ = \frac{P(w) P(y|x,w)}{P(y|x)}$, we assume w is independent of x.

\subsection*{Example MAP for lin. Gaussian}
$ \underset{w}{\operatorname{argmax}} P(w|x,y) = $\\ 
$\underset{w}{\operatorname{argmin}} - log P(w) - log P(y|x,w) + const.$ \\
$= \underset{w}{\operatorname{argmin}} \frac{1}{2\beta^2} ||w||_2^2 + \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - w^Tx_i)^2$ \\
$= \underset{w}{\operatorname{argmin}} \lambda ||w||_2^2 + \sum_{i=1}^n (y_i - w^Tx_i)^2$ , w/ $\lambda = \frac{\sigma^2}{\beta^2}$

\subsection*{Logistic regression}
Link function: $\sigma(w^Tx) = \frac{1}{1+exp(-w^Tx)}$\\
Derivate: $\frac{\partial}{\partial z} \sigma(z) = \sigma(z)(1-\sigma(z))$\\
Logistic regression replaces the assumption of Gaussian noise by iid Bernoulli noise.\\
$P(y|x,w) = Ber(y; \sigma(w^Tx))$\\
$= \left \{
	\begin{array}{lr}
		$1/(1+exp(-w^Tx)) = \sigma(w^T x)$\\
		$1 - 1/(1+exp(-w^Tx)) = \sigma (-w^T x)$\\
	\end{array}
$\\
Learning: $w = \underset{w}{\operatorname{argmax}} P(w|x,y)$\\
Classification: Use $P(y|x,w) = \frac{1}{1+exp(-yw^Tx)}$ and predict most likely class label.

\subsection*{Example: MLE for logistic regression}
$\underset{w}{\operatorname{argmax}} P(y|x,w) = \underset{w}{\operatorname{argmin}} - \sum_{i=1}^n log P(y|x_i,w)$\\
$\underset{w}{\operatorname{argmin}} \sum_{i=1}^n log(1+exp(-yw^Tx_i))$

\subsection*{Gradient for logistic regression}
Loss function $l(w) = log(1+exp(-yw^Tx))$\\
$\nabla_w l(w) = \frac{1}{1+exp(-yw^Tx)} exp(-yw^Tx) (-yx)$\\
$=\frac{1}{1+exp(+yw^Tx)} (-yx)$\\
$=P(-y|x,w) (-yx)$

\subsection*{SGD for logistic regression}
1. Initialize w\\
2. For t=1,2,...\\
Pick data $(x,y) \in_{u.a.r} D$\\
Compute probability of misclassification $P(-y|w,x) = \frac{1}{1+exp(yw^Tx)}$\\
Update step $w = w + \eta_t y x P(-y|w,x)$

\subsection*{Logistic regression and regularization}
L2 (Gaussian prior):\\
$\underset{w}{\operatorname{min}} \sum_{i=1}^n log(1+exp(-y_i w^T x_i)) + \lambda ||w||_2^2$\\
L1 (Laplace):\\
$\underset{w}{\operatorname{min}} \sum_{i=1}^n log(1+exp(-y_i w^T x_i)) + \lambda ||w||_1$\\

\subsection*{SGD for L2 logistic regression}
Update step $w = w(1-2\lambda \eta_t) + \eta_t y x P(-y|w,x)$