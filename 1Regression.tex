% -*- root: Main.tex -*-
\section*{Regression}
\subsection*{Linear Regression}
Error: $\hat{R}(w) = \sum_{i=1}^n (y_i - w^Tx_i)^2 = ||Xw-y||^2_2$\\
Closed form: $w^*=(X^T X)^{-1} X^T y$\\
Gradient: $\nabla_w \hat{R}(w) = -2 \sum_{i=1}^n (y_i-w^T x_i) \cdot x_i = 2X^T (Xw-y)$

\subsection*{Convex}
$\text{g(x) is convex}$\\
$\Leftrightarrow x_1,x_2 \in \mathbb{R}, \lambda \in [0,1]:$\\
$g(\lambda x_1) + (1-\lambda x_2) \leq \lambda g(x_1) + (1-\lambda) g(x_2)$
$ \Leftrightarrow g''(x) > 0$

\subsection*{Jensen's inequality}}
X is a random variable \& $\varphi$ a convex function then the following holds 
$\varphi(\mathbb{E}[X]) \leq \mathbb{E}[\varphi(X)]$

\subsection*{Gradient Descent}
1. Start arbitrary $w_o \in \mathbb{R}$\\
2. For $i$ do $w_{t+1} = w_t - \eta_t \nabla \hat{R}(w_t)$


\subsection*{Expected Error}
For generalization, minimize the expected error
$R(w) = \int P(x,y) (y-w^Tx)^2 \partial x \partial y$\\
$= \mathbb{E}_{x,y}[(y-w^Tx)^2]$

\subsection*{Gaussian distribution}
Standard deviation $\sigma$ \\
Mean $\mu$ \\
$f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{- \frac{1}{2} \frac{(x-\mu)^2}{\sigma^2}}$

\subsection*{Multivariate Gaussian}
Covariance matrix $\Sigma$ \\
Mean $\mu$ \\
$f(x) = \frac{1}{2\pi \sqrt{|\Sigma|}} e^{- \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)}$
\subsection*{Ridge regression}
Error: $\hat{R}(w) = \sum \limits_{i=1}^n (y_i - w^Tx_i)^2 + \lambda ||w||_2^2$\\
Closed form: $w^*=(X^T X + \lambda I)^{-1} X^T y$\\
Gradient: $\nabla_w \hat{R}(w) = -2 \sum \limits_{i=1}^n (y_i-w^T x_i) \cdot x_i + 2 \lambda w$

\subsection*{Regularization}
The error term $L$ and the regularization $C$ with regularization parameter $\lambda$: $\min \limits_w L(w) + \lambda C(w)$\\
L1-regularization for number of features \\
L2-regularization for the length of $w$
